{"/docs/":{"data":{"":" Installation Administration Working with Spaces Example Spaces Utilities "},"title":"Documentation"},"/docs/administration/":{"data":{"":" Volumes Variables Templates Groups Users "},"title":"Administration"},"/docs/administration/api-tokens/":{"data":{"":"API tokens are created automatically when logging in with the knot client. Tokens can also be created manually and used to access the API from external applications.","creating-a-token#Creating a Token":"From the menu select API Tokens, then click Create Token.\nComplete the Name field and click Create Token, a new token is generated and the list of available tokens is displayed.\n⚠️ Tokens expire after a week of inactivity, any API call will reset the lifespan of the tokens. ","deleting-a-token#Deleting a Token":"Deleting a token instantly stops further API calls being made using the token.\nFrom the API token list click Delete next to the token to delete, and confirm the operation when prompted."},"title":"API Tokens"},"/docs/administration/groups/":{"data":{"":"","overview#Overview":"know allows groups to be defined, groups can be used to limit which templates each developer can see. For example web developers may need PHP environments but have no need for a Go development environment used by DevOps.\nFor this example two groups can be defined, Web Developers and DevOps, to define the groups from the Groups page select Create Group:\nOnce the groups have been defined the templates can be edited and assigned to the appropriate groups, users can also be edited and assigned to the appropriate group.\nBoth users and templates can be assigned multiple groups.","template-groups#Template Groups":"When creating or editing templates the groups that the template will be visible to can be selected. If not groups are selected then the template will be available to users in all groups.","user-groups#User Groups":"When creating or editing users the groups that the user belongs to can be set.\nWhen a user creates a space the templates that either have no groups or groups that overlap with that users groups will be shown."},"title":"Groups"},"/docs/administration/templates/":{"data":{"":"","creating-a-template#Creating a Template":"From the menu select Templates then Create Template, the following form is displayed:\nThe Name and Nomad Job fields are required, the Nomad Job field takes an HCL job specification, see example environments.\nℹ️ When a change is made to a template all running spaces are marked as an update available, however the spaces are not automatically restarted. Once the spaces are restarted they will receive the updated template. Template variables can be used to hold registry login information e.g.\nimage = \"paularlott/knot-debian:12\" auth { username = \"${{ .var.registry_user }}\" password = \"${{ .var.registry_pass }}\" } ⚠️ Variables are plain text within the Nomad template and can therefore be viewed via the Nomad web interface, and alternative solution such at Vault may be more applicable depending on the environment. ","deleting-a-template#Deleting a Template":"If a template is in use then it can’t be deleted.","overview#Overview":"Each developer environment or Template, is defined by a Nomad job and optional volume definition.\nWhen a developer creates an instance of the Template, a space, and starts it, knot automatically creates any required volumes and launches the job within the Nomad cluster.\nFor additional isolation between developers the namespace can be set within the job specification, e.g. namespace=\"${{ .user.username }}\", if this is done then jobs for each developer are placed within their own namespaces."},"title":"Templates"},"/docs/administration/users/":{"data":{"":"","creating-a-user#Creating a User":"From the menu select Users and then Create User:\nThe Preferred Shell is used when the user opens a web based terminal into the space, the system will attempt to open the selected shell and if not found will look for another available shell. This can be changed per space when creating a space.\nWhen connecting to a space via SSH and the client, the SSH Public Key if set will be passed to the space to allow password less logins. The user should set the SSH Public Key themselves by clicking their username in the top right.\nTimezone is used within the spaces to set their timezones, clicking or typing in the field will generate a searchable list of available timezones.\nMaximum Spaces is the maximum number of spaces that the user can create, when set to 0 there’s no limit applied.\nMaximum Disk Space is the maximum disk space the user can use in GB, when set to 0 there’s no limit applied. Volumes created outside the space Template are not included in the space used by the user.\nRoles is the optional list of roles to assign a user, if no roles are assigned then the user will only have the ability to start spaces and interact with their spaces.\nGroups defines the list of groups that a user will belong to, only templates that have no groups assigned or have groups overlapping those of the user will be available when creating a new space.","deleting-a-user#Deleting a User":" ⚠️ When a user is deleted any spaces created by the user are also deleted and any data in associated volumes is lost. Select the menu next to the user to delete, click Delete and confirm the action.","editing-a-user#Editing a User":"Editing the user is similar to creating a new user, however it the password fields are left blank then the password for the user is left unchanged."},"title":"User Management"},"/docs/administration/variables/":{"data":{"":"","available-variables#Available Variables":" Group Name Description space space.id The UUID of the space space.name The name of the space template template.id The UUID of the template used to create the space template.name The name of the template used to create the space user user.id The UUID of the user running the space user.timezone The timzone of the user user.username The username of the user running the space user.email The users email address user.service_password Service password for the user server server.url The URL of the knot server server.agent_url The URL of the knot server that agents should use server.wildcard_domain The wildcard domain without the leading * ","system-variables#System Variables":"System variables are accessible for both job templates and volume templates.\nTo make use of a system variable it simply needs to specified, for example, ${{ .space.name }}.","user-defined-variables#User Defined Variables":"User-defined variables can be created through the web interface. These variables are then made available for use in both job and volume templates.\nTo use a user-defined variable, for example myvariable, it must be prefixed with .var.. Therefore, the correct usage within a template would be ${{ .var.myvariable }}.\nWhen creating or editing a variable the Protected option can be selected. If a variable is marked as protected then when editing the variable the value isn’t loaded back into the browser. Protected variables are stored encrypted in the database but are decrypted before being used in templates therefore their values my be exposed within the Nomad job definitions."},"title":"Variables"},"/docs/administration/volumes/":{"data":{"":"","space-volumes#Space Volumes":"Each development environment template can define one or more volumes, if volumes are defined then they are created when the environment is deployed and destroyed when the environment is destroyed.\nℹ️ Starting and stopping the environment does not affect the lifespan of the volumes. ⚠️ Deleting the space will destroy the volumes and all data on them. An example volume definition that allocates block storage for two volumes home and data would look like:\nvolumes: - id: \"${{.space.id}}_home\" name: \"${{.space.id}}_home\" plugin_id: \"hostpath\" capacity_min: 1G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" - id: \"${{.space.id}}_data\" name: \"${{.space.id}}_data\" plugin_id: \"hostpath\" capacity_min: 1G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" In the example the ID of the volume is given as ${{.space.id}}_home, .space.id is replaced with the unique ID of the space using the volume.\nIf volume definitions are added or removed from the space template then those volumes are created or destroyed the next time the space is started. Any data in a volume being deleted will be lost.","standalone-volumes#Standalone Volumes":"Volumes can be created and managed independently to spaces. This flexibility allows for a volume to be created and then attached to multiple spaces, such as providing storage for /home.\nThese standalone volumes are not impacted by the lifespan of the spaces they’re attached to. Even if all spaces are deleted, these standalone volumes, along with their data, will continue to persist.\nAny Container Storage Interface (CSI) driver supported by Nomad can be used, knot places no additional requirements on this.\nCreating a Volume From the menu select Volumes then click Create Volume the following form will be presented:\nThe name field is a descriptive name for the volume, it’s not used within the volume definition. The Volume Definition field takes YAML and expects a single volume to be defined. If more than one volume is defined then the volume will not be able to be started.\nThe following defines a volume named test_home:\nvolumes: - id: \"test_home\" name: \"test_home\" plugin_id: \"hostpath\" capacity_min: 10G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" The description of the values can be found in the Nomad Volume Specification.\nOnce the name and definition have been entered click Create Volume to define the volume.\nStarting a Volume Only once a volume has been started will it be available within the cluster.\nFrom the Volumes page click the menu next to the volume to start and select Start.\nStopping a Volume ⚠️ Stopping a volume will destroy all data on the volume. Stopping a volume will free the resources that it is using within the Nomad cluster, this will destroy all data on the volume.\nDeleting a Volume Only stopped volumes can be destroyed, from the dropdown menu next to the volume select Delete and confirm the choice."},"title":"Volumes"},"/docs/example-spaces/":{"data":{"":" Debian Ubuntu "},"title":"Example Spaces"},"/docs/example-spaces/debian/":{"data":{"":"The following defines a simple Debian space with a volume attached to the home directory, the volume uses hostpath CSI driver which is assumes has been configured within the nomad cluster.\nThe space provides an instance of code-server which can be accessed in the browser via the knot web interface.\nNomad-Jobjob \"${{.space.name}}-${{.user.username}}\" { datacenters = [\"dc1\"] update { max_parallel = 1 min_healthy_time = \"30s\" healthy_deadline = \"1m\" auto_revert = true } group \"debian\" { count = 1 network { port \"knot_port\" { to = 3000 } } volume \"home_volume\" { type = \"csi\" source = \"debian_${{.space.id}}_home\" read_only = false attachment_mode = \"file-system\" access_mode = \"single-node-writer\" } task \"debian\" { driver = \"docker\" config { image = \"paularlott/knot-base-debian:bookworm\" ports = [\"knot_port\"] hostname = \"${{ .space.name }}\" } env { # Define environment variables for agent KNOT_SERVER = \"${{.server.url}}\" KNOT_SPACEID = \"${{.space.id}}\" KNOT_SSH_PORT = \"22\" KNOT_CODE_SERVER_PORT = \"49374\" KNOT_USER = \"${{.user.username}}\" TZ = \"${{ .user.timezone }}\" } volume_mount { volume = \"home_volume\" destination = \"/home\" } resources { cores = 4 memory = 4096 } # Publish Agent Port service { name = \"knot-${{.space.id}}\" port = \"knot_port\" check { name = \"alive\" type = \"http\" protocol = \"https\" tls_skip_verify = true path = \"/ping\" interval = \"10s\" timeout = \"2s\" } } } } } Volume-Definitionvolumes: - id: \"debian_${{.space.id}}_home\" name: \"debian_${{.space.id}}_home\" plugin_id: \"cephrbd\" capacity_min: 10G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" secrets: userID: \"admin\" userKey: \"FWef0320r23rmvseE+oke2CXEwiifWODSaoqp4==\" parameters: clusterID: \"3abdeec0-ae9c-477b-ab36-d4e3c20e86d0\" pool: \"rbd\" imageFeatures: \"deep-flatten,exclusive-lock,fast-diff,layering,object-map\" If the namespace is set on the job e.g. to ${{.user.username}} then all the spaces would be placed into a namespace of the username, with the correct nomad configuration this would allow users to access nomad but only interact with their jobs.","startup-scripts#Startup Scripts":"During the startup of the container any scripts found in the /etc/knot-startup.d/ directory are executed as root, then any scripts in the .knot-startup.d/ directory within the users home directory are executed as the user.\nThis allows for both system level scripts to be started and user specific scripts."},"title":"Debian"},"/docs/example-spaces/ubuntu/":{"data":{"":"The following defines a simple Ubuntu 22.04 space with a volume attached to the home directory, the volume uses hostpath CSI driver which is assumes has been configured within the nomad cluster.\nThe space provides an instance of code-server which can be accessed in the browser via the knot web interface.\nNomad-Jobjob \"${{.space.name}}-${{.user.username}}\" { datacenters = [\"dc1\"] update { max_parallel = 1 min_healthy_time = \"30s\" healthy_deadline = \"1m\" auto_revert = true } group \"ubuntu\" { count = 1 network { port \"knot_port\" { to = 3000 } } volume \"home_volume\" { type = \"csi\" source = \"ubuntu_${{.space.id}}_home\" read_only = false attachment_mode = \"file-system\" access_mode = \"single-node-writer\" } task \"ubuntu\" { driver = \"docker\" config { image = \"paularlott/knot-base-ubuntu:22.04\" ports = [\"knot_port\"] hostname = \"${{ .space.name }}\" } env { # Define environment variables for agent KNOT_SERVER = \"${{.server.url}}\" KNOT_SPACEID = \"${{.space.id}}\" KNOT_SSH_PORT = \"22\" KNOT_CODE_SERVER_PORT = \"49374\" KNOT_USER = \"${{.user.username}}\" TZ = \"${{ .user.timezone }}\" } volume_mount { volume = \"home_volume\" destination = \"/home\" } resources { cores = 4 memory = 4096 } # Publish Agent Port service { name = \"knot-${{.space.id}}\" port = \"knot_port\" check { name = \"alive\" type = \"http\" protocol = \"https\" tls_skip_verify = true path = \"/ping\" interval = \"10s\" timeout = \"2s\" } } } } } Volume-Definitionvolumes: - id: \"ubuntu_${{.space.id}}_home\" name: \"ubuntu_${{.space.id}}_home\" plugin_id: \"cephrbd\" capacity_min: 10G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" secrets: userID: \"admin\" userKey: \"FWef0320r23rmvseE+oke2CXEwiifWODSaoqp4==\" parameters: clusterID: \"3abdeec0-ae9c-477b-ab36-d4e3c20e86d0\" pool: \"rbd\" imageFeatures: \"deep-flatten,exclusive-lock,fast-diff,layering,object-map\" If the namespace is set on the job e.g. to ${{.user.username}} then all the spaces would be placed into a namespace of the username, with the correct nomad configuration this would allow users to access nomad but only interact with their jobs.","startup-scripts#Startup Scripts":"During the startup of the container any scripts found in the /etc/knot-startup.d/ directory are executed as root, then any scripts in the .knot-startup.d/ directory within the users home directory are executed as the user.\nThis allows for both system level scripts to be started and user specific scripts."},"title":"Ubuntu"},"/docs/install/":{"data":{"":" System Requirements Client Install Server Install Initial User Setup "},"title":"Installation"},"/docs/install/client/":{"data":{"":"While the client software is not mandatory - given that spaces are easily accessible through the web interface - installing it unlocks a range of enhanced functionalities. Once installed, the client facilitates SSH access into active spaces, enables port forwarding, and offers an array of additional features.\nOpting for the install of the client maximize your capabilities and streamlines your interaction with the system.\nLinuxmacOSWindows The Linux client can be installed via Homebrew or by downloading the latest binary from GitHub releases.\nbrew install paularlott/tap/knot The preferred install method for macOS is via Homebrew\nbrew install paularlott/tap/knot Alternatively the latest binary can be downloaded from GitHub releases.\nThe latest binary can be downloaded from GitHub releases "},"title":"Client Install"},"/docs/install/initial-user/":{"data":{"":"Once the server has been deployed point the browser at the URL defined in the configuration file, the following form will be presented:\nOnce the form has been completed a new user will be created within the database and installation will be completed. Once this has happened the login screen will be presented.\nAfter logging into the system clicking the username in the top right will show the user profile, the profile should be checked and adjusted as required."},"title":"Initial User Setup"},"/docs/install/server/":{"data":{"":"knot requires the deployment of a server component. This component is responsible for overseeing the user environments, managing users, and regulating access to active spaces. There are two options available for deploying the server: it can be installed on a virtual machine or deployed as a Nomad job. The rest of this guide focuses on the Nomad based deployment.","configuration#Configuration":"datacenters will need updating for the cluster that the job is being deployed to as will the tags so that the ingress controller can route traffic to the server.\nThe knot server is configured via the local/knot.yml file:\nknot.ymllog: level: info server: listen: 0.0.0.0:3000 download_path: /srv url: \"https://knot.example.com\" wildcard_domain: \"*.knot.example.com\" encrypt: \"knot genkey\" mysql: database: knot enabled: true host: \"\" password: \"\" user: \"\" nomad: addr: \"http://nomad.service.consul:4646\" token: \"\" The configuration should be updated:\nurl The host the server will be accessed as wildcard_domain The wildcard domain used to provide web access to the containers web server encrypt The encryption key for encrypting variables, this is generated with knot genkey mysql.* The configuration information for the MySQL server to use nomad.* The configuration for communicating with Nomad, the token must have permission to access any namespaces used in environment jobs The MySQL database should be empty as on first run knot will create the required tables and initialize the data.\nCaching A redis server can be used along side MySQL to store the state of the agents, this is more performant than storing the agent state in the database.\nTo enable caching simply add a redis configuration e.g.:\nknot.ymllog: level: info server: listen: 0.0.0.0:3000 download_path: /srv url: \"https://knot.example.com\" wildcard_domain: \"*.knot.example.com\" encrypt: \"knot genkey\" mysql: database: knot enabled: true host: \"\" password: \"\" user: \"\" redis: enabled: true host: srv+redis.service.consul password: \"\" db: 0 nomad: addr: \"http://nomad.service.consul:4646\" token: \"\" In this mode all data is stored in MySQL only the agent states are stored in redis.\nWithout MySQL Redis The knot server can be run using a Redis server by replacing knot.yml in the nomad job:\nknot.ymllog: level: info server: listen: 0.0.0.0:3000 download_path: /srv url: \"https://knot.example.com\" wildcard_domain: \"*.knot.example.com\" encrypt: \"knot genkey\" redis: enabled: true host: srv+redis.service.consul password: \"\" db: 0 nomad: addr: \"http://nomad.service.consul:4646\" token: \"\" BadgerDB The knot server can be run without MySQL by using the embedded BadgerDB by replacing knot.yml in the nomad job:\nknot.ymllog: level: info server: listen: 0.0.0.0:3000 download_path: /srv url: \"https://knot.example.com\" wildcard_domain: \"*.knot.example.com\" encrypt: \"knot genkey\" badgerdb: enabled: true path: /data/ nomad: addr: \"http://nomad.service.consul:4646\" token: \"\" The /data/ directory must be mounted to persistent storage or configurations are not persisted between restarts.\n⚠️ When using BadgerDB only one instance of the knot server can be started and pointed at the BadgerDB data directory. ","the-job-file#The Job File":"The latest example of the job file can be generated by running the following command:\nknot scaffold --nomad The output of the command will generate the following, this can’t be directly deployed as it needs updating to match the Nomad cluster and MySQL server.\nknot-server.nomadjob \"knot-server\" { datacenters = [\"dc1\"] update { max_parallel = 1 min_healthy_time = \"30s\" healthy_deadline = \"1m\" auto_revert = true } group \"knot-server\" { count = 1 network { port \"knot_port\" { to = 3000 } } task \"knot-server\" { driver = \"docker\" config { image = \"paularlott/knot:latest\" ports = [\"knot_port\"] } env { KNOT_CONFIG = \"/local/knot.yml\" } template { data = \u003c\u003cEOF log: level: info server: listen: 0.0.0.0:3000 download_path: /srv url: \"https://knot.example.com\" wildcard_domain: \"*.knot.example.com\" encrypt: \"knot genkey\" mysql: database: knot enabled: true host: \"\" password: \"\" user: \"\" nomad: addr: \"http://nomad.service.consul:4646\" token: \"\" EOF destination = \"local/knot.yml\" } resources { cpu = 256 memory = 512 } # Knot Agent Port service { name = \"knot-${NOMAD_JOB_NAME}\" port = \"knot_port\" # Expose the port on a domain name # tags = [ # \"urlprefix-knot.example.com proto=https tlsskipverify=true\", # \"urlprefix-*.knot.example.com proto=https tlsskipverify=true\" # ] check { name = \"alive\" type = \"http\" protocol = \"https\" tls_skip_verify = true path = \"/health\" interval = \"10s\" timeout = \"2s\" } } } } } "},"title":"Server Install"},"/docs/install/system-requirements/":{"data":{"":" Running Nomad cluster Database Server, MySQL recommended for production but the embedded BadgerDB database or a Redis server can be used Ingress Controller e.g. fabio Storage for Home Directories, ideally this will be controlled via CSI drivers within Nomad "},"title":"System Requirements"},"/docs/utilities/":{"data":{"":" Direct Connections Proxied Connections "},"title":"Utilities"},"/docs/utilities/direct-connection/":{"data":{"":"","lookup#Lookup":"The lookup command queries for the IP and port associated with a service.\nknot direct lookup example.service.consul ","overview#Overview":"If the knot client is installed then it provides functionality to allow connection to services running within a Nomad cluster using the service names.\nAll subcommands can accept the --nameserver parameter, if given then the specified nameserver will be queried for the service name.","port-forwarding#Port Forwarding":"Port forwarding will forward a local port to the port associated with the service running within the Nomad cluster.\nknot direct port :9000 example.service.consul The browser can then be pointed at http://localhost:9000","ssh-forwarding#SSH Forwarding":"SSH forwarding will forward a SSH connection to a SSH server running within the Nomad cluster identified by the service name.\nssh -o ProxyCommand='knot direct ssh %h' -o StrictHostKeyChecking=no user@mytest.service.consul "},"title":"Direct Connection"},"/docs/utilities/proxied-connections/":{"data":{"":"","lookup#Lookup":"The lookup command queries for the IP and port associated with a service.\nknot proxy lookup example.service.consul ","overview#Overview":"If the knot client is installed and the knot server is started with the option --enable-proxy then it provides functionality to allow connections to services running within a Nomad cluster to be proxied via knot, this can allow connection to services without the local machine having direct access to the cluster network.","port-forwarding#Port Forwarding":"Port forwarding will forward a local port to the port associated with the service running within the Nomad cluster.\nknot proxy port :9000 example.service.consul The browser can then be pointed at http://localhost:9000","ssh-forwarding#SSH Forwarding":"SSH forwarding will forward a SSH connection to a SSH server running within the Nomad cluster identified by the service name.\nssh -o ProxyCommand='knot proxy ssh %h' -o StrictHostKeyChecking=no user@mytest.service.consul "},"title":"Proxied Connections"},"/docs/working-with-spaces/":{"data":{"":" Managing a Space Web Based Terminal Visual Studio Code Web Server Port Forwarding SSH Access "},"title":"Working with Spaces"},"/docs/working-with-spaces/code-server/":{"data":{"":"\nTo start the web based Visual Studio Code click the Code Server icon next to the running space, if this icon isn’t available then the agent isn’t detecting code-server running within the container. When the icon is clicked a new window is opened the the code-server.\nInitially code-server is started without any configuration or plugins, however these can all be added as they would be in the desktop version. For more information on code-server please see the code-server project page.\nAssuming the template uses a volume for /home/ then changes to code-server are persistent across restarts."},"title":"Visual Studio Code"},"/docs/working-with-spaces/managing/":{"data":{"":"","creating-a-space#Creating a Space":"From the Templates page open the menu next to the template to use and click Create Space:\nℹ️ Depending on the permissions the user has the option Create Space For maybe displayed, clicking this will prompt for the user under which the space is to be created. This allows an admin to create spaces for users. The fllowing form will be presented:\nEnter a name for the space e.g. mytest and leave the Terminal Shell as Bash, once Create Space is clicked the space will be created within knot and the main spaces page loaded.\nThe space is created in a stopped state, no resources are used within the Nomad cluster at this point.\nThe Additional Space Names section allows additional names to be entered against the space, this is useful when using the web proxy service and development needs to access the target software under multiple domain names.\nManual Spaces It’s possible to run the agent manually on a virtual machine or even a physical server and connect to it from the knot web interface.\nSelect the Manual-Configuration template, then fill out the URL of the agent e.g. http://192.168.0.1:3000 if the address can be found via a DNS SRV lookup then the URL can be given in the form srv+http://vm.service.consul.","deleting-a-space#Deleting a Space":" 🚫 Deleting a space will delete the volumes and any data they contain. Only stopped spaces can be deleted.\nFrom the menu next to the stopped space select the Delete item and confirm deletion. When the space is deleted all resources are freed from the Nomad cluster and all volumes and their associated data are removed.","example-template#Example Template":"This section assumes a template name mytest has been defined as follows:\nNomad-Jobjob \"${{.space.name}}-${{.user.username}}\" { datacenters = [\"dc1\"] update { max_parallel = 1 min_healthy_time = \"30s\" healthy_deadline = \"1m\" auto_revert = false } group \"debian\" { count = 1 network { port \"knot_port\" { to = 3000 } } volume \"home_volume\" { type = \"csi\" source = \"debian_${{.space.id}}_home\" read_only = false attachment_mode = \"file-system\" access_mode = \"single-node-writer\" } task \"debian\" { driver = \"docker\" config { image = \"paularlott/knot-debian:12\" ports = [\"knot_port\"] } env { # Define environment variables for agent KNOT_SERVER = \"${{ .server.url }}\" KNOT_SPACEID = \"${{ .space.id }}\" KNOT_SSH_PORT = \"22\" KNOT_TCP_PORT = \"80\" KNOT_HTTP_PORT = \"80\" KNOT_CODE_SERVER_PORT = \"49374\" KNOT_LOGLEVEL = \"debug\" KNOT_USER = \"${{ .user.username }}\" TZ = \"${{ .user.timezone }}\" } volume_mount { volume = \"home_volume\" destination = \"/home\" } resources { cpu = 300 memory = 512 } # Knot Agent Port service { name = \"knot-${{ .space.id }}\" port = \"knot_port\" check { name = \"alive\" type = \"http\" protocol = \"https\" tls_skip_verify = true path = \"/ping\" interval = \"10s\" timeout = \"2s\" } } } } } Volume-Definitionvolumes: - id: \"debian_${{.space.id}}_home\" name: \"debian_${{.space.id}}_home\" plugin_id: \"cephrbd\" capacity_min: 10G capacity_max: 10G mount_options: fs_type: \"ext4\" mount_flags: - rw - noatime capabilities: - access_mode: \"single-node-writer\" attachment_mode: \"file-system\" secrets: userID: \"admin\" userKey: \"FWef0320r23rmvseE+oke2CXEwiifWODSaoqp4==\" parameters: clusterID: \"3abdeec0-ae9c-477b-ab36-d4e3c20e86d0\" pool: \"rbd\" imageFeatures: \"deep-flatten,exclusive-lock,fast-diff,layering,object-map\" ","service-password#Service Password":"A service password can be set by going My Profile and entering the chosen password in the Service Password field, if not set the system will generate a random password.\nThis can then be used in templates as the variable ${{ .user.service_password }}, e.g. for a MariaDB server it can be used as the root password by setting the MARIADB_ROOT_PASSWORD environment variable MARIADB_ROOT_PASSWORD = \"${{.user.service_password}}\".\nℹ️ If the password is change the new password isn’t immediately made available to the spaces, however it will be used on the next start of the space. ","starting-a-space#Starting a Space":"From the Spaces page click the menu item next to the space to start, and then select Start, them menu will change to read “Starting” and after a few seconds the Running will show in the Status column.\nThe environment will continue its boot process during which time additional icons will appear next to the space, e.g. Terminal.\nNot all icons will appear for all spaces as they are dependant on the agent configuration within the space.\nSSH Is shown when it’s possible to create a SSH connection to the space, clicking the icon will show the command line information for connecting to the space. Code Server Is shown if a running instance of Visual Studio Code is found running within the space, clicking the icon opens a new tab or window showing the editor. Terminal Is shown if a web based terminal can be opened into the space, clicking the icon opens a new window showing the terminal. Ports Is shown if there’s ports exposed that can either be connected to via the web interface or via port forwarding on the command line. Clicking the icon drops down a list of the available ports, ports shown with a solid background can be connected to by clicking the button and will open in a new tab or window, while ports with an outline are available for use with port forwarding on the command line. Desktop Is show if a running web based VNC server such as KasmVNC is available within the container. Clicking it will open a new window displaying the graphical desktop. ","stopping-a-space#Stopping a Space":"Clicking the menu item next to the running space will show the Stop button.\n⚠️ When stopping a space all data in memory and not on a persistent volume will be lost. However any volumes used by the space will not be deleted. ","updating-a-space#Updating a Space":"If the template that a running space is using is updated then an Update Available badge is displayed:\nTo update the space, stop it and then start it again. Add volumes that have been added to the template will be created when the space starts and any volumes that have been removed from the template will be deleted along with the data they contain.\nA space can also be edited, this allows changing of the space name as well as updating any additional names for the space. Additional URLs are supported as soon as the space is successfully saved."},"title":"Managing a Space"},"/docs/working-with-spaces/port-forwarding/":{"data":{"":"Port forwarding requires the knot client be installed on the local computer as it will forward a local port to the port within the remote container.\nClicking the Ports icon next to a running space will show the list of ports that are exposed for forwarding, they are shown as outlined numbers.\nThis example expects to follow on from the Web Server example where caddy was installed and started on port 80.\nOn the client machine connect to the knot server, replacing the URL with the address of the real server, first open a terminal and run:\nknot connect https://knot.example.com When the command runs the browser is opened and if not logged in you will need to login, copy the generated token and paste into terminal.\nNext forward port 9010 on the local host to port 80 on the space called mytest by running the command:\nknot forward port 127.0.0.1:9010 mytest 80 Point a web browser to http://127.0.0.1:9010 if everything works correctly the caddy welcome page will open in the browser.\nℹ️ Ports forwarded in this way require authentication and are not publicly available. "},"title":"Port Forwarding"},"/docs/working-with-spaces/ssh/":{"data":{"":"","adding-a-public-ssh-key#Adding a Public SSH Key":"Click your username in the top right of the web interface and paste your public SSH key into the field SSH Public Key, then click Update User. This will set the public key to be used by all your containers when connecting via SSH to allow authentication without passwords.","agent-forwarding#Agent Forwarding":"Adding the -A option to the ssh command will enable agent forwarding which allows SSH keys from the local machine to be used within the space.\nThe current list of keys exported can be found by running the following on the local machine:\nssh-add -L SSH keys can be added with ssh-add YOUR-KEY\nOn macOS the ssh-agent will forget the key once it is restarted, e.g. the machine is rebooted, however the key can be added to the keychain with the --apple-use-keychain option, ssh-add --apple-use-keychain YOUR-KEY.","connecting-via-ssh#Connecting via SSH":"SSH access requires the knot client be installed on the local computer as it will forward the SSH session to the remote container.\nIf not already done, on the client machine connect to the knot server, replacing the URL with the address of the real server, first open a terminal and run:\nknot connect https://knot.example.com When the command runs the browser is opened and if not logged in you will need to login, copy the generated token and paste into terminal.\nNext open a SSH connection to the space called mytest by running the command below (adjust the username as required):\nssh -o ProxyCommand='knot forward ssh %h' -o StrictHostKeyChecking=no user@mytest The SSH session will be opened to the container and can be used as per any other SSH connection.","using-sshconfig#Using .ssh/config":"To shorten the command for connecting to the remote space the following can be added to the .ssh/config file of the local computer:\n.ssh/configHost mytest HostName mytest StrictHostKeyChecking = no ProxyCommand knot forward ssh %h Once this is done a SSH connection can be opened with:\nssh user@mytest "},"title":"SSH Access"},"/docs/working-with-spaces/terminal/":{"data":{"":"\nFrom the Spaces page click the Terminal icon next to the space to connect to this will open a terminal in a new browser window.\nWhen the terminal opens it will attempt to use the shell specified in the space configuration, however if that shell isn’t available within the container then the knot agent will attempt to find an alternative client, the search order is bash, zsh, fish and sh.\nClosing the window closes the terminal."},"title":"Web Based Terminal"},"/docs/working-with-spaces/web-server/":{"data":{"":"Using a space created from the mytest template the first step is to install a web server, for this example caddy will be used.\nOpen a web terminal and follow the instructions at https://caddyserver.com/docs/install#debian-ubuntu-raspbian\nsudo apt update -y sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https curl curl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg curl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list sudo apt update -y sudo apt install caddy Start caddy with the defaults:\nsudo caddy start --config /etc/caddy/Caddyfile Click the Ports icon next to the running space and click the port 80 on the solid background, this is the port that is exposed via the web interface of knot.\nOnce clicked a new tab or windows will be opened showing the default welcome page for caddy.\nDepending on the template caddy or another web server may already be present or require specific steps to launch the web server.\n⚠️ Ports exposed via the web interface do not require authentication to access them. "},"title":"Web Server"}}